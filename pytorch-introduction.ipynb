{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone to the deep learning course.\n",
    "\n",
    "This introduction notebook should give you a start to work with PyTorch, one of the standard deep learning frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import all necessary libraries.\n",
    "\n",
    "If you get an error that something is missing please install\n",
    "\n",
    "- **Matplotlib** via `pip install matplotlib`\n",
    "- **Numpy** via `pip install numpy`\n",
    "- **PyTorch** according to the installation guide on [their website](https://pytorch.org/). Note that there is a difference between CPU/GPU (cuda) installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy is the standard numerical library for Python\n",
    "import numpy as np\n",
    "\n",
    "# torch is the deep learning library we are using in this course\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CUDA support (not necessary for this course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test now whether you installed PyTorch with GPU support (won't be the case if you run it on a machine without NVIDIA GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x = torch.zeros().cuda()\n",
    "    print(\"gpu support works\")\n",
    "except:\n",
    "    print(\"no gpu found or pytorch has been installed without cuda support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical computations with tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch behaves very similar to numpy. You can define tensors (= high dimensional matrices) which you can then use in numerical computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Arithmetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.])      # define two tensors x and y\n",
    "y = torch.tensor([2.])\n",
    "z = x + y                   # z is the sum of both\n",
    "print(f\"z is {z}\")          # this will print the tensor itself (you will read something like tensor([3.]))\n",
    "print(f\"z is {z.item()}\")   # this will print the value stored within the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.tensor([                # we can also define matrix types\n",
    "    [1., 1.],\n",
    "    [-1., 1.]\n",
    "])\n",
    "v = torch.tensor([3., 1.])        # and vectors\n",
    "u = M @ v                         # and compute u via applying M on v\n",
    "print(f\"u is {u}\")                # show the value of u\n",
    "# print(f\"u is {u.item()}\")       # this will fail now as u is no scalar type anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher-Order Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = torch.tensor([               # higher-order tensors are possible as well\n",
    "    [\n",
    "        [1., 1.],\n",
    "        [1., 1.]\n",
    "    ],\n",
    "    [\n",
    "        [1., 1.],\n",
    "        [1., 1.]\n",
    "    ],\n",
    "    [\n",
    "        [1., 1.],\n",
    "        [1., 1.]\n",
    "    ],\n",
    "])\n",
    "print(f\"L has shape {L.shape}\")  # we can print its shape to see what dimensionality it has\n",
    "\n",
    "u = torch.matmul(L, v)           # matrix vector computation still works \n",
    "print(f\"u has shape {u.shape}\")\n",
    "\n",
    "K = torch.einsum(\"a b c, b c -> a\", L, M)   # we can even do complicated tensor contractions\n",
    "print(f\"K has shape {K.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-Wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3., 4.])\n",
    "\n",
    "y = x.square()                         # beyond vector-matrix-tensor reductions we also have a range of pointwise functions\n",
    "                                       # those are applied to tensor elements individually\n",
    "print(f\"y is {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many many functions that you can use with tensors. We recommend to have a look at PyTorch's [excellent API documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 2., 3., 4]])\n",
    "print(f\"x has shape {x.shape}\")\n",
    "\n",
    "y = torch.tensor([[1.], [2.], [3.], [4.]])\n",
    "print(f\"y has shape {y.shape}\")\n",
    "\n",
    "z = x + y                             # broadcasting allows us to simplify arithmetic operations / tensor reductions \n",
    "                                      # for tensors with different shapes, if the axes match. here all elements of x\n",
    "                                      # and all elements of y are summed pointwise and their result is stored within a\n",
    "                                      # 4x4 tensor\n",
    "print(f\"z has shape {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running computations on the GPU to speed up highly-parallel computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 500\n",
    "A = torch.randn(dim, dim)          # create two very big tensors\n",
    "B = torch.randn(dim, dim, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "C = torch.einsum(\"a b, a b c -> c\", A, B)  # do some expensive tensor contraction on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    A = A.cuda()                                # send all tensors to the GPU\n",
    "    B = B.cuda()\n",
    "else:\n",
    "    print(\"This won't work if you did not install PyTorch with CUDA support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "if torch.cuda.is_available():\n",
    "    C = torch.einsum(\"a b, a b c -> c\", A, B)   # this should be much faster now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides trivialized GPU support for numeric computation the more important reason to use PyTorch for Machine Learning is the support of [**Automatic Differentiation**](https://en.wikipedia.org/wiki/Automatic_differentiation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we want to compute the derivative of a Python function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have three options:\n",
    "\n",
    "**Option 1:** We compute the derivative `f'` by hand and code it ourselves\n",
    "\n",
    "- **Pros**: \n",
    "    - We can optimize the implementation by hand.\n",
    "    - It is fast and exact.\n",
    "- **Cons**: \n",
    "    - We have to do this for every function by hand. \n",
    "    - This becomes nasty for complicated expressions.\n",
    "    - It becomes impossible for dynamically generated code (e.g. if `f` contains a for loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2:** We approximate the derivative `f'` via [finite differences](https://en.wikipedia.org/wiki/Finite_difference)\n",
    "\n",
    "- **Pros**: This is a very generic implementation that works for every function\n",
    "- **Cons**: \n",
    "    - Becomes slow in high dimensions.\n",
    "    - We have to choose some epsilon.\n",
    "    - It is **not exact**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_derivative(f, x, epsilon=1e-2):\n",
    "    return ( f(x + epsilon) - f(x - epsilon) ) / (2 * epsilon)\n",
    "\n",
    "x = torch.tensor(3.)\n",
    "d1 = f_prime(x)\n",
    "d2 = finite_difference_derivative(f, x)\n",
    "\n",
    "print(f\"d1 is {d1.item()}, d2 is {d2.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3 (preferred):** We compute the derivative of `f` via [**Automatic Differentiation**](https://en.wikipedia.org/wiki/Automatic_differentiation). Here we implement `f'` for each primitive function (addition, sine, matrix multiplication, ...) and compute the derivative of complex nested functions made of those primitives using the [chain-rule](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "- **Pros**: Get derivatives for complex nested functions for free with one algorithm. It's fast and **it's exact**!\n",
    "- **Cons**: \n",
    "    - Implement derivatives for each primitive (however the existing library contains most imaginable).\n",
    "    - A buch of details are necessary to make this method efficient (those are hidden under the hood of PyTorch and you barely have to care about them in most cases).\n",
    "   \n",
    "PyTorch implements [**Reverse-Mode Automatic Differentation**](https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation). If you are curious about the details, have a look [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "The most important take-away is, that you can compute *Vector-Jacobian products* efficiently, namely expressions of the form $v^T \\frac{d f}{d x}$ where $v$ is some vector and $f$ is some Python function.\n",
    "\n",
    "Let's look at the example to see how that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.).requires_grad_(True)    # this tells PyTorch that x should now be considered for derivatives during automatic differentation\n",
    "y = f(x)                                     # now we just compute the result of f as we are used to it in standard Python\n",
    "\n",
    "print(f\"y is {y.item()}\")\n",
    "\n",
    "v = torch.tensor(1.)                         # we define the v as introduce above - in the languate of Autograd this is called the `output derivative`\n",
    "\n",
    "d3, = torch.autograd.grad(                   # this is where the magic happens. torch.autograd.grad computes v^T df/dx for us. \n",
    "    outputs=y,                               # We feed in the output of the computation y = f(x)\n",
    "    inputs=x,                                # The input wrt we want to take derivatives (d/dx)\n",
    "    grad_outputs=v                           # And the output derivative. If we set it to 1 we should get the usual derivative.\n",
    ")                                            # Important remark: for conventional reasons the result of torch.autograd.grad is a tuple.\n",
    "                                             # So we unpack it to get the actual derivative.\n",
    "\n",
    "print(f\"d3 is {d3.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why are interested in derivatives is that a lot in machine learning deals with optimizing functions wrt parameters. You will learn the theory about it in the lecture.\n",
    "\n",
    "Let's assume we are given a somewhat complicated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return  (x * np.pi).cos() * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at it by plotting it with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y = f(x)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is now to approximate this function using a set of polynomials: $f(x) \\approx \\sum\\limits_{i=0}^{N} a_{i} \\cdot x^{i}$.\n",
    "\n",
    "For maximum degree $N$ the coefficients $\\{ a_i | i=0\\ldots N\\} $ are free parameters that we have to tune.\n",
    "\n",
    "While there exist much more principled approaches to polynomial regression, we choose to find the optimal $a_i$ using [**Gradient Descent**](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "Gradient descent allows us to find $\\theta^{*}$ which minimizes a loss function $\\ell(\\theta)$. It works as a local and step-wise procedure:\n",
    "\n",
    "1. Start with an initial guess for $\\theta$. Call it $\\theta^{(0)}$.\n",
    "2. Evaluate $\\ell(\\theta^{(i)}$.\n",
    "3. Compute $\\frac{d \\ell(\\theta^{(i)}}{d \\theta}$.\n",
    "4. Update $\\theta^{(i+1)} = \\theta^{(i)} - \\alpha \\cdot \\frac{d \\ell(\\theta^{(i)}}{d \\theta}$. Here $\\alpha$ is the *learning rate* which is a small number.\n",
    "5. If not converged, set $i = i +1$ and continue with 2.\n",
    "6. Set $\\theta^{*} = \\theta^{(i)}$\n",
    "\n",
    "Enough theory, let's implement it in PyTorch!\n",
    "\n",
    "As a first function we implement the polynomials themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_polynomial(coeffs, x):\n",
    "    N = coeffs.shape[0]\n",
    "    y_accum = torch.zeros_like(y)         # this makes y_accum a zero initialized tensor with the same shape as x\n",
    "    x_accum = torch.ones_like(x)          # this makes x_accum a one initialized tensor with the same shape as x\n",
    "    for i in range(N):\n",
    "        y_accum = y_accum + coeffs[i] * x_accum\n",
    "        x_accum = x_accum * x\n",
    "    return y_accum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot some polynomials to see how they look like. We just use randomly initalized coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "\n",
    "for i in range(6):                       \n",
    "    coeffs = torch.randn(N+1)\n",
    "    x = torch.linspace(-1, 1, 100)\n",
    "    y = evaluate_polynomial(coeffs, x)\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our optimization procedure we need to define a loss function which compares our current fit against the target function.\n",
    "\n",
    "Here we choose a simple mean-squared error that is evaluated point-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    return (y_true - y_pred).square().mean()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our parameters. PyTorch offers a class `torch.nn.Parameter` that eats a tensor and marks it as a parameter that can be optimized over plus some other convenience options.\n",
    "E.g. parameters can be used for taking gradients by default - you don't need to call `.requires_grad_(True)` on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = torch.nn.Parameter(               \n",
    "    torch.randn(N+1) / N\n",
    ")\n",
    "print(f\"coeffs are {coeffs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let look at our initial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y_pred = evaluate_polynomial(coeffs, x).detach()\n",
    "y_true = f(x) \n",
    "plt.plot(x, y_pred, label=\"prediction\")\n",
    "plt.plot(x, y_true, label=\"target\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PyTorch's implemented gradient descent optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optim = torch.optim.SGD(params=[coeffs], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the optimization.\n",
    "\n",
    "We will just randomly sample some points in $[-1, 1]$, evaluate the target function and the polynomial for them, compute the mean-square error, compute it's derivative and them run one step of gradient-descent.\n",
    "\n",
    "Luckily, PyTorch offers a lot to make life easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100                                  # number of samples we use in each iteration\n",
    "n_iterations = 50_000                            # number of gradient descent iterations\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    optim.zero_grad()                            # makes sure that the automatic differentiation buffer is clean - this step is important!\n",
    "    x = torch.rand(n_samples) * 2 - 1            # sample 100 points in [-1, 1]\n",
    "    y_pred = evaluate_polynomial(coeffs, x)      # compute our current estimate of the polynomial\n",
    "    y_true = f(x)                                # compute the actual values for the same xs\n",
    "    loss = loss_function(y_true, y_pred)         # compute the mean-squared error loss\n",
    "    \n",
    "    loss.backward()                              # This is a magic function of PyTorch. It will compute the gradients of `loss` with respect to all `torch.nn.Parameters` that appear in the computation.\n",
    "                                                 # Theoretically, we could have used something like coeffs_grad, = torch.autograd.grad(loss, coeffs) and computed the gradient descent step explicitly.\n",
    "                                                 # However, as this becomes very tedious once your model contains many parameter tensors (e.g. deep neural networks) and your optimizers become more sophisticated.\n",
    "                                                 \n",
    "    optim.step()                                 # run one step of gradient descent\n",
    "    \n",
    "    print(f\"iteration {it}, mean-squared error is {loss.item():.4}\", end=\"\\r\")  # print current loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y_pred = evaluate_polynomial(coeffs, x).detach()\n",
    "y_true = f(x) \n",
    "plt.plot(x, y_pred, label=\"prediction\")\n",
    "plt.plot(x, y_true, label=\"target\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit is not very great. There is a good reason, why better methods for polynomial regression exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and composability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simple variables and Python functions becomes messy very quickly. For this reason PyTorch allows us to compose functions with parameters into `Modules` which can be composed and reused without any pain.\n",
    "\n",
    "As an illustration of how that works, we will try to fit the same function as before, but now using a slightly different model. \n",
    "\n",
    "Here we choose a kernel-regression given by $f(x) \\approx \\sum\\limits_{i=0}^{N} a_{i} \\phi_{i}(x)$. Where the $\\phi_i$ are different *kernel-functions*.\n",
    "\n",
    "Our polynomials from before are a possible choice of kernel functions. However, now we choose radial basis functions of the form $\\phi_i(x) = \\exp(-\\sigma_{i} \\cdot (x - \\mu_{i})^2)$. In addition to our coefficients $a_i$ we now have parameters $\\sigma_i$ and $\\mu_i$ to fit.\n",
    "\n",
    "As before we try to do that via gradient descent, however now we are using [**PyTorch Modules**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) to structure our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RbfBasis(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, mu_init, log_sigma_init):\n",
    "        super().__init__()                                       # this line should always be the first in the module constructor\n",
    "        self.mu = torch.nn.Parameter(mu_init)                    # we bind parameters to the module by simple member assignment\n",
    "        self.log_sigma = torch.nn.Parameter(log_sigma_init)\n",
    "\n",
    "    def forward(self, x):                # Each module has a `forward` method. It implements the logic of what happens when you apply the module like a function, e.g. like module(x)\n",
    "                                         # It helps to think of modules as functions with an internal state (parameters). The `forward` method implements what is returned after calling the function.\n",
    "        \n",
    "        x = x.unsqueeze(-1)              # reshape x into  shape [n_samples, 1]\n",
    "        mu = self.mu.unsqueeze(0)        # reshape mu into shape [1, N]\n",
    "        diff = x - mu                    # then we can compute x - mu via broadcasting. This will have shape [n_samples, N].\n",
    "        \n",
    "        sigma = self.log_sigma.exp()     # to avoid negative sigmas we parameterize their log-values instead\n",
    "        \n",
    "        return torch.exp(-sigma * diff.square())   # returns the [n_samples, N] tensor containing the kernel basis\n",
    "    \n",
    "    \n",
    "class KernelRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, basis, coeffs_init):\n",
    "        super().__init__()\n",
    "        self.basis = basis\n",
    "        self.coeffs = torch.nn.Parameter(coeffs_init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        basis_functions = self.basis(x)        # compute basis functions\n",
    "        return basis_functions @ self.coeffs   # apply coefficients and sum up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some randomly initialized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "\n",
    "for i in range(6):                       \n",
    "\n",
    "\n",
    "    rbf_basis = RbfBasis(\n",
    "        mu_init=torch.randn(N) * 2 - 1,\n",
    "        log_sigma_init=torch.randn(N)\n",
    "    )\n",
    "    model = KernelRegression(\n",
    "        basis=rbf_basis,\n",
    "        coeffs_init=torch.randn(N) \n",
    "    )\n",
    "    \n",
    "    x = torch.linspace(-1, 1, 100)\n",
    "    y = model(x).detach()\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N= 6\n",
    "\n",
    "rbf_basis = RbfBasis(\n",
    "    mu_init=torch.randn(N) * 2 - 1,\n",
    "    log_sigma_init=torch.randn(N)\n",
    ")\n",
    "model = KernelRegression(\n",
    "    basis=rbf_basis,\n",
    "    coeffs_init=torch.randn(N) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the initial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y_pred = model(x).detach()\n",
    "y_true = f(x) \n",
    "plt.plot(x, y_pred, label=\"prediction\")\n",
    "plt.plot(x, y_true, label=\"target\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use another optimizer called `Adam`. You will learn more about it in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again run the optimization loop as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "n_iterations = 50_000\n",
    "\n",
    "for it in range(n_iterations):\n",
    "    optim.zero_grad()                            \n",
    "    x = torch.rand(n_samples) * 2 - 1            \n",
    "    y_pred = model(x)      \n",
    "    y_true = f(x)                                \n",
    "    loss = loss_function(y_true, y_pred)         \n",
    "    loss.backward()                                                                          \n",
    "    optim.step()                                 \n",
    "    print(f\"iteration {it}, mean-squared error is {loss.item():.4}\", end=\"\\r\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the final fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y_pred = model(x).detach()\n",
    "y_true = f(x) \n",
    "plt.plot(x, y_pred, label=\"prediction\")\n",
    "plt.plot(x, y_true, label=\"target\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot our learned basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "basis = model.basis(x)\n",
    "\n",
    "for y in basis.T:\n",
    "    plt.plot(x, y.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** if CUDA works for your PyTorch setup, you can try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.cuda()    # send your module and thus all its internally stored parameters and buffer to the GPU\n",
    "    x = torch.randn(100000, device=\"cuda\")\n",
    "    y = model(x)\n",
    "except:\n",
    "    print(\"pytorch seems to be installed without cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice you can now try the following extensions of the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Implement the Polynomial basis from the section *Parameters and optimization* as a `torch.nn.Module`, such that you can use it in the constructor of `KernelRegression` instead of the `rbf_basis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialBasis(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        # your code here\n",
    "    \n",
    "    def forward(self, ...):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Try to extend the code, such that your regressions fits the following function $g \\colon \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ in the interval $[0, 1] \\times [0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    # assume x to be of shape [n_samples, 2]\n",
    "    assert x.shape[1] == 2, \"input should be of shape [n_samples, 2]\"\n",
    "    x = x * np.pi\n",
    "    y = torch.cat([\n",
    "        x[:, 0].cos() * x[:, 1].sin(),\n",
    "        x[:, 0].sin() * x[:, 1].cos()\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
